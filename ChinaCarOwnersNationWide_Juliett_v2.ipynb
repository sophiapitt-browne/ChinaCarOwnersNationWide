{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zVLPrjIjqx3"
      },
      "source": [
        "# All Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii7iDeWXjpcm"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import os\n",
        "import logging\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gjZIrI1kSj5"
      },
      "source": [
        "# Declare All Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lQOFeO0kWnl"
      },
      "outputs": [],
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# prompt: Create a function to read a specified CSV, drop columns from a dataframe based on a list of specified columns and convert the revised dataframe to a specified CSV.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def process_drop_cols_csv(input_csv, output_csv, columns_to_drop):\n",
        "  \"\"\"Reads a CSV, drops specified columns, and writes the result to another CSV.\n",
        "\n",
        "  Args:\n",
        "    input_csv: Path to the input CSV file.\n",
        "    output_csv: Path to the output CSV file.\n",
        "    columns_to_drop: A list of column names to drop from the dataframe.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    print(f\"Processing CSV file ({input_csv}). Columns to be dropped are {columns_to_drop}\")\n",
        "    logging.info(f\"Processing CSV file ({input_csv}). Columns to be dropped are {columns_to_drop}\")\n",
        "\n",
        "    df = pd.read_csv(input_csv, encoding='utf-8',low_memory=True)\n",
        "    df = df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
        "    df.to_csv(output_csv, encoding='utf-8-sig',index=False)\n",
        "\n",
        "    print(f\"CSV file ({input_csv}) processed successfully. Output saved to {output_csv}\")\n",
        "    logging.info(f\"CSV file ({input_csv}) processed successfully. Output saved to {output_csv}\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Input file not found at {input_csv}\")\n",
        "    logging.error(f\"Error: Input file not found at {input_csv}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    logging.error(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# prompt: Create a function to remove duplicate records based on specified columns. Add the duplicates records to separate dataframe and drop them from the original. Include error checking and logging.\n",
        "\n",
        "def remove_duplicate_records(df, columns):\n",
        "    \"\"\"\n",
        "    Removes duplicate records based on specified columns.\n",
        "    Adds duplicate records to a separate dataframe and drops them from the original.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe to process.\n",
        "        columns (list): A list of column names to consider for duplicate detection.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with unique records\n",
        "               and a new dataframe with duplicate records.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Processing dataframe to remove and store duplicate records.\")\n",
        "        logging.info(f\"Processing dataframe to remove and store duplicate records.\")\n",
        "        duplicate_df = pd.DataFrame()\n",
        "        df_deduplicated = df.drop_duplicates(subset=columns, keep='first')\n",
        "        duplicate_rows = df[~df.index.isin(df_deduplicated.index)]\n",
        "\n",
        "        if not duplicate_rows.empty:\n",
        "            duplicate_df = pd.concat([duplicate_df, duplicate_rows], ignore_index=True)\n",
        "\n",
        "        print(f\"Duplicate removal complete. Duplicate records appended to duplicate_df.\")\n",
        "        logging.info(f\"Duplicate removal complete. Duplicate records appended to duplicate_df.\")\n",
        "        return df_deduplicated, duplicate_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during duplicate removal: {e}\")\n",
        "        logging.error(f\"Error occurred during duplicate removal: {e}\")\n",
        "        return df, pd.DataFrame()\n",
        "\n",
        "# prompt: Create a function to process a specified CSV file and then run the function to remove duplicates and convert the valid and duplicates dataframes to csv files.\n",
        "\n",
        "def process_duplicates_csv(file_path, output_valid_csv, output_duplicates_csv, columns, sep=','):\n",
        "    \"\"\"\n",
        "    Processes a single CSV file, removes duplicates, and outputs valid and duplicate dataframes to CSV files.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, sep=sep, low_memory=True, encoding='utf-8')\n",
        "        print(f\"Processing CSV file: {file_path} for duplicates.\")\n",
        "        logging.info(f\"Processing CSV file: {file_path} for duplicates.\")\n",
        "\n",
        "        # Remove duplicates based on email\n",
        "        df, duplicates_df = remove_duplicate_records(df, columns)\n",
        "\n",
        "        # Save valid and duplicate dataframes to CSV files\n",
        "        df.to_csv(output_valid_csv, index=False, encoding='utf-8-sig')\n",
        "        duplicates_df.to_csv(output_duplicates_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"Processed file: {file_path}. Valid data saved to {output_valid_csv}, duplicates to {output_duplicates_csv}.\")\n",
        "        logging.info(f\"Processed file: {file_path}. Valid data saved to {output_valid_csv}, duplicates to {output_duplicates_csv}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "\n",
        "# prompt: Create a function to split a large csv into chunks in a specified folder or path using the chunksize parameter in read_csv\n",
        "\n",
        "def split_csv_into_chunks(file_path, chunksize, output_directory, sep=','):\n",
        "  \"\"\"Splits a large CSV file into smaller chunks using the chunksize parameter.\n",
        "\n",
        "  Args:\n",
        "    file_path: The path to the large CSV file.\n",
        "    chunksize: The number of rows per chunk.\n",
        "    output_directory: The directory where the chunks should be saved.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    print(f\"Processing CSV file: {file_path} to split into chunks.\")\n",
        "    logging.info(f\"Processing CSV file: {file_path} to split into chunks.\")\n",
        "\n",
        "    if not os.path.exists(output_directory):\n",
        "      os.makedirs(output_directory)\n",
        "\n",
        "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, sep=sep, encoding='utf-8')):\n",
        "      output_file = os.path.join(output_directory, f\"chunk_{i+1}.csv\")\n",
        "      chunk.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(f\"File '{file_path}' split into {i+1} chunks in '{output_directory}'.\")\n",
        "    logging(f\"File '{file_path}' split into {i+1} chunks in '{output_directory}'.\")\n",
        "\n",
        "  except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    logging.error(f\"Error: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred when splitting CSV into chunks: {e}\")\n",
        "    logging.error(f\"An unexpected error occurred when splitting CSV into chunks: {e}\")\n",
        "\n",
        "# prompt: Create a function to get chunked CSVs from a specified folder, runs the validation functions and outputs the cleaned chunks in specified folders. Include error checking and logging.\n",
        "\n",
        "def process_chunked_csvs_output_folders(input_folder, output_valid_folder, output_error_folder, email_column_name='email', date_columns=['created_at']):\n",
        "  \"\"\"\n",
        "  Processes chunked CSV files from a specified folder, runs validation functions,\n",
        "  and outputs the cleaned chunks in specified folders. Includes error checking and logging.\n",
        "\n",
        "  Args:\n",
        "      input_folder (str): The path to the folder containing chunked CSV files.\n",
        "      output_valid_folder (str): The path to the folder to output cleaned chunks.\n",
        "      output_error_folder (str): The path to the folder to output chunks with errors.\n",
        "      email_column_name (str): The name of the email column.\n",
        "      date_columns (list): A list of column names to consider for date validation.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Setup logging\n",
        "    #logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    print(f\"Processing chunks from: {input_folder} for data cleaning and output to folders.\")\n",
        "    logging.info(f\"Processing chunks from: {input_folder} for data cleaning and output to folders.\")\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "      if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "        logging.info(f\"Processing file: {file_path}\")\n",
        "\n",
        "        try:\n",
        "          df = pd.read_csv(file_path, low_memory=True, encoding='utf-8')\n",
        "\n",
        "          # Run validation functions\n",
        "          df = validate_email_dataframe(df, email_column_name,'noemail')\n",
        "          columns_to_combine = ['address', 'province', 'city', 'postal_code']\n",
        "          new_column_name = 'full_address'\n",
        "          df = combine_columns(df, columns_to_combine, new_column_name)\n",
        "          columns_to_check = ['vehicle_identification_number', 'id_card_number']\n",
        "          df, chunk_error_df = validate_alphanumeric_columns(df, columns_to_check)\n",
        "\n",
        "          # Output cleaned chunk\n",
        "          if not os.path.exists(output_valid_folder):\n",
        "            os.makedirs(output_valid_folder)\n",
        "\n",
        "          output_valid_file = os.path.join(output_valid_folder, f\"valid_{filename}\")\n",
        "          df.to_csv(output_valid_file, index=False, encoding='utf-8-sig')\n",
        "          print(f\"Final valid data saved to {output_valid_file}.\")\n",
        "          logging.info(f\"Final valid data saved to {output_valid_file}.\")\n",
        "\n",
        "          # Output chunk with errors\n",
        "          if not os.path.exists(output_error_folder):\n",
        "            os.makedirs(output_error_folder)\n",
        "\n",
        "          output_error_file = os.path.join(output_error_folder, f\"error_{filename}\")\n",
        "          chunk_error_df.to_csv(output_error_file, index=False, encoding='utf-8-sig')\n",
        "          print(f\"Final error data saved to {output_error_file}.\")\n",
        "          logging.info(f\"Final error data saved to {output_error_file}.\")\n",
        "\n",
        "          print(f\"File from {file_path} processed successfully.\")\n",
        "          logging.info(f\"File {file_path} processed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f\"Error processing file {file_path}: {e}\")\n",
        "          logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Critical error during processing: {e}\")\n",
        "    logging.critical(f\"Critical error during processing: {e}\")\n",
        "\n",
        "# prompt: Create a function to check whether the specified columns contain alphanumerical characters only and if they don't save the invalid records to a dataframe, dropthe invalid record from the original dataframe. Return both the cleaned and invalid records dataframes.\n",
        "\n",
        "def validate_alphanumeric_columns(df, columns_to_validate):\n",
        "    \"\"\"Checks if specified columns contain only alphanumeric characters and separates invalid records.\n",
        "\n",
        "    Args:\n",
        "      df: The pandas DataFrame.\n",
        "      columns_to_validate: A list of column names to validate.\n",
        "\n",
        "    Returns:\n",
        "      A tuple containing two DataFrames: (cleaned_df, invalid_records_df)\n",
        "    \"\"\"\n",
        "    invalid_records_df = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        for column in columns_to_validate:\n",
        "            if column not in df.columns:\n",
        "                print(f\"Column '{column}' not found in DataFrame.\")\n",
        "                logging.error(f\"Column '{column}' not found in DataFrame.\")\n",
        "                raise KeyError(f\"Column '{column}' not found in DataFrame.\")\n",
        "\n",
        "            logging.info(f\"Validating column '{column}' for alphanumeric characters.\")\n",
        "\n",
        "            # Try to convert column to string and check for alphanumeric characters\n",
        "            try:\n",
        "                invalid_rows = df[~df[column].astype(str).str.isalnum()]\n",
        "                invalid_records_df = pd.concat([invalid_records_df, invalid_rows])\n",
        "                df = df[df[column].astype(str).str.isalnum()]\n",
        "\n",
        "                print(f\"Finished validating column '{column}'. Invalid rows found: {len(invalid_rows)}.\")\n",
        "                logging.info(f\"Finished validating column '{column}'. Invalid rows found: {len(invalid_rows)}.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing column '{column}': {e}\")\n",
        "                logging.error(f\"Error processing column '{column}': {e}\")\n",
        "                raise ValueError(f\"Error processing column '{column}': {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred during validation.\")\n",
        "        logging.exception(\"An error occurred during validation.\")\n",
        "        raise e\n",
        "\n",
        "    return df, invalid_records_df\n",
        "\n",
        "# prompt: Create a function to remove the time from a date in specified columns\n",
        "\n",
        "def remove_time_from_date(df, columns):\n",
        "  \"\"\"Removes the time component from date columns in a DataFrame.\n",
        "\n",
        "  Args:\n",
        "    df: The DataFrame containing the date columns.\n",
        "    columns: A list of column names to process.\n",
        "\n",
        "  Returns:\n",
        "    The DataFrame with the time component removed from the specified columns.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    print(f\"Removing time from specified date columns: {columns}\")\n",
        "    logging.info(f\"Removing time from specified date columns: {columns}\")\n",
        "    for column in columns:\n",
        "      if column in df.columns:\n",
        "        # Convert to datetime if not already\n",
        "        df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "        # Remove the time component\n",
        "        df[column] = df[column].dt.date\n",
        "    print(\"Date cleaning complete. Time removed from date columns.\")\n",
        "    logging.info(\"Date cleaning complete. Time removed from date columns.\")\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred during removing time from date function: {e}\")\n",
        "    logging.error(f\"An error occurred during removing time from date function: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# prompt: create a function to validate emails addresses in a dataframe, append the records with an invalid email address to a dataframe and drop them from the original dataframe. Return both the updated dataframe and the error dataframe. Include error checking and loggging.\n",
        "\n",
        "def validate_and_remove_invalid_emails(df, email_column):\n",
        "    \"\"\"\n",
        "    Validates email addresses in a dataframe, appends records with invalid email\n",
        "    addresses to a new dataframe, and removes them from the original dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataframe containing email addresses.\n",
        "        email_column (str): The name of the column containing email addresses.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the updated dataframe with valid email addresses\n",
        "            and a new dataframe with records containing invalid email addresses.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Removing invalid emails from {email_column}.\")\n",
        "        logging.info(f\"Removing invalid emails from {email_column}.\")\n",
        "\n",
        "        # Regular expression for basic email validation\n",
        "        email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "\n",
        "        # Create a new dataframe to store records with invalid email addresses\n",
        "        error_df = pd.DataFrame()\n",
        "\n",
        "        # Iterate through the dataframe and validate email addresses\n",
        "        for index, row in df.iterrows():\n",
        "            email = row[email_column]\n",
        "            if not re.match(email_regex, email):\n",
        "                # Append record to the error dataframe\n",
        "                error_df = pd.concat([error_df, pd.DataFrame([row])], ignore_index=True)\n",
        "                # Drop the record from the original dataframe\n",
        "                df.drop(index, inplace=True)\n",
        "\n",
        "        print(\"Validation complete. Invalid email records appended to error_df.\")\n",
        "        logging.info(\"Validation complete. Invalid email records appended to error_df.\")\n",
        "        return df, error_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred during email validation: {e}\")\n",
        "        logging.error(f\"Error occurred during email validation: {e}\")\n",
        "        return df, pd.DataFrame()  # Return empty error dataframe in case of error\n",
        "\n",
        "# prompt: create a function to validate email addresses in a dataframe, all values in the column should be set as lower case, option to see the value as null if it matches a specified string or if the email address is invalid. Return the updated dataframe. Include error checking.\n",
        "\n",
        "def validate_email_dataframe(df, email_column, null_if_match=None):\n",
        "    \"\"\"Validates email addresses in a DataFrame column and optionally sets them to null.\n",
        "\n",
        "    Args:\n",
        "      df: The pandas DataFrame.\n",
        "      email_column: The name of the column containing email addresses.\n",
        "      null_if_match: An optional string. If an email address matches this string\n",
        "        or is invalid, it will be set to null.\n",
        "\n",
        "    Returns:\n",
        "      The updated DataFrame with validated email addresses.\n",
        "\n",
        "    Raises:\n",
        "      KeyError: If email_column is not found in the DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Check if the column exists in the DataFrame\n",
        "        if email_column not in df.columns:\n",
        "            print(f\"Column '{email_column}' not found in DataFrame.\")\n",
        "            logging.error(f\"Column '{email_column}' not found in DataFrame.\")\n",
        "            raise KeyError(f\"Column '{email_column}' not found in DataFrame.\")\n",
        "\n",
        "        print(f\"Validating emails in column '{email_column}'.\")\n",
        "        logging.info(f\"Validating emails in column '{email_column}'.\")\n",
        "\n",
        "        # Function to validate email addresses\n",
        "        def validate_email(email):\n",
        "            try:\n",
        "                if email is None or pd.isnull(email):\n",
        "                    return None\n",
        "                email = str(email).lower()  # Convert to lowercase\n",
        "\n",
        "                # Check if the email matches the 'null_if_match' string\n",
        "                if null_if_match and null_if_match in str(email):\n",
        "                    #logging.info(f\"Email '{email}' matches null_if_match condition, setting to None.\")\n",
        "                    return None\n",
        "\n",
        "                # Regex pattern for valid email\n",
        "                pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
        "                if re.fullmatch(pattern, email):\n",
        "                    return email\n",
        "                else:\n",
        "                    #logging.warning(f\"Email '{email}' is invalid, setting to None.\")\n",
        "                    return None\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error validating email '{email}': {e}\")\n",
        "                logging.error(f\"Error validating email '{email}': {e}\")\n",
        "                return None\n",
        "\n",
        "        # Apply the validation to the email column\n",
        "        df[email_column] = df[email_column].apply(validate_email)\n",
        "        print(f\"Finished validating emails in column '{email_column}'.\")\n",
        "        logging.info(f\"Finished validating emails in column '{email_column}'.\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"A KeyError occurred: {e}\")\n",
        "        logging.exception(\"A KeyError occurred: {e}.\")\n",
        "        raise e\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while validating emails: {e}\")\n",
        "        logging.exception(f\"An error occurred while validating emails: {e}\")\n",
        "        raise e\n",
        "\n",
        "    return df\n",
        "\n",
        "# Combine the list of specified columns into a new column\n",
        "def combine_columns(df, columns_to_combine, new_column_name, separator=\" \"):\n",
        "  \"\"\"Combines multiple columns into a single column in a DataFrame.\n",
        "\n",
        "  Args:\n",
        "    df: The pandas DataFrame.\n",
        "    columns_to_combine: A list of column names to combine.\n",
        "    new_column_name: The name of the new column.\n",
        "    separator: The string used to separate values from different columns.\n",
        "\n",
        "  Returns:\n",
        "    The updated DataFrame with the combined column and old columns dropped.\n",
        "\n",
        "  Raises:\n",
        "    KeyError: If any of the specified columns are not found in the DataFrame.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    print(f\"Combining columns: {columns_to_combine} into {new_column_name}.\")\n",
        "    logging.info(f\"Combining columns: {columns_to_combine} into {new_column_name}.\")\n",
        "    df[new_column_name] = df[columns_to_combine].apply(lambda row: separator.join(row.dropna().astype(str)), axis=1)\n",
        "    df = df.drop(columns=columns_to_combine)\n",
        "    print(f\"Combined columns:{columns_to_combine} successfully.\")\n",
        "    logging.info(f\"Combined columns: {columns_to_combine} successfully.\")\n",
        "\n",
        "    return df\n",
        "  except KeyError as e:\n",
        "    print(f\"Error: Column(s) not found in DataFrame: {e}.\")\n",
        "    logging.error(f\"Error: Column(s) not found in DataFrame: {e}.\")\n",
        "    raise e\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred while combining columns: {e}.\")\n",
        "    logging.error(f\"An error occurred while combining columns: {e}.\")\n",
        "    raise e\n",
        "\n",
        "# prompt: Create a function to get chunked csvs from a specified folder, runs the validation functions and merges the chunks into a specified final valid csv file and final error csv file. INclude error checking and logging.\n",
        "\n",
        "def process_chunked_csvs(input_folder, output_valid_csv, output_error_csv, email_column_name='email', date_columns=['created_at']):\n",
        "  \"\"\"\n",
        "  Processes chunked CSV files from a specified folder, runs validation functions,\n",
        "  and merges the results into final valid and error CSV files.\n",
        "\n",
        "  Args:\n",
        "      input_folder (str): The path to the folder containing chunked CSV files.\n",
        "      output_valid_csv (str): The path to the output CSV file for valid records.\n",
        "      output_error_csv (str): The path to the output CSV file for error records.\n",
        "      output_duplicates_csv (str): The path to the output CSV file for duplicate records.\n",
        "      email_column_name (str): The name of the email column.\n",
        "      date_columns (list): A list of column names to consider for date validation.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Setup logging\n",
        "    #logging.basicConfig(filename='processing_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    valid_df = pd.DataFrame()\n",
        "    error_df = pd.DataFrame()\n",
        "\n",
        "    print(f\"Processing chunks for data cleaning & combining from {input_folder} to single cleaned file: {output_valid_csv}\")\n",
        "    logging.info(f\"Processing chunks for data cleaning & combining from {input_folder} to single cleaned file: {output_valid_csv}\")\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "      if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        print(f\"Processing chunk: {file_path}\")\n",
        "        logging.info(f\"Processing chunk: {file_path}\")\n",
        "\n",
        "        try:\n",
        "          df = pd.read_csv(file_path, low_memory=True, encoding='utf-8')\n",
        "\n",
        "          # Run validation functions\n",
        "          df = validate_email_dataframe(df, email_column_name,'noemail')\n",
        "          columns_to_combine = ['address', 'province', 'city', 'postal_code']\n",
        "          new_column_name = 'full_address'\n",
        "          df = combine_columns(df, columns_to_combine, new_column_name)\n",
        "          columns_to_check = ['vehicle_identification_number', 'id_card_number']\n",
        "          df, chunk_error_df = validate_alphanumeric_columns(df, columns_to_check)\n",
        "\n",
        "          valid_df = pd.concat([valid_df, df], ignore_index=True)\n",
        "\n",
        "          # Concatenate error dataframes\n",
        "          #chunk_error_df = pd.concat([chunk_error_df,chunk_dup_error_df], ignore_index=True)\n",
        "          error_df = pd.concat([error_df, chunk_error_df], ignore_index=True)\n",
        "\n",
        "          print(f\"File {file_path} processed successfully.\")\n",
        "          logging.info(f\"File {file_path} processed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "          print(f\"Error processing file {file_path}: {e}\")\n",
        "          logging.error(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Remove duplicates from full dataframe\n",
        "    #valid_df, duplicates_df = remove_duplicate_records(valid_df, ['mail_address']) #remove duplicates based on email\n",
        "    #error_df = pd.concat([error_df, chunk_dup_error_df], ignore_index=True)\n",
        "\n",
        "    # Save final dataframes\n",
        "    valid_df.to_csv(output_valid_csv, index=False, encoding='utf-8-sig')\n",
        "    error_df.to_csv(output_error_csv, index=False, encoding='utf-8-sig')\n",
        "    #duplicates_df.to_csv(output_duplicates_csv, index=False)\n",
        "\n",
        "    print(f\"Final cleaned data saved to {output_valid_csv}.\")\n",
        "    logging.info(f\"Final cleaned data saved to {output_valid_csv}.\")\n",
        "    print(f\"Final garbage data saved to {output_error_csv}.\")\n",
        "    logging.info(f\"Final garbage data saved to {output_error_csv}.\")\n",
        "    #logging.info(f\"Final duplicates data saved to {output_duplicates_csv}.\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Critical error during processing of chunks: {e}\")\n",
        "    logging.critical(f\"Critical error during processing of chunks: {e}\")\n",
        "\n",
        "# prompt: Create a function that will combine csv chunks from a specified folder into one csv file\n",
        "\n",
        "def combine_csv_chunks(input_folder, output_file):\n",
        "  \"\"\"\n",
        "  Combines multiple CSV chunks from a folder into a single CSV file.\n",
        "\n",
        "  Args:\n",
        "    input_folder: The path to the folder containing CSV chunks.\n",
        "    output_file: The path to the output CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  combined_df = pd.DataFrame()\n",
        "  print(f\"Combining CSV chunks from {input_folder}\")\n",
        "  logging.info(f\"Combining CSV chunks from {input_folder}\")\n",
        "\n",
        "  for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".csv\"):\n",
        "      file_path = os.path.join(input_folder, filename)\n",
        "      try:\n",
        "        df = pd.read_csv(file_path, encoding='utf-8')\n",
        "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "      except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        logging.error(f\"Error reading file {file_path}: {e}\")\n",
        "\n",
        "  combined_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "  print(f\"Combined CSV chunks saved to {output_file}\")\n",
        "  logging.info(f\"Combined CSV chunks saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck7XaHgC8jDI"
      },
      "source": [
        "# **Optional Step: Function to remove invalid rows from a CSV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvyt3sIN8pcW"
      },
      "outputs": [],
      "source": [
        "# prompt: Create a function that removes invalid rows from a CSV. Detect the number of columns based on the header and the specified delimiter.\n",
        "# Rows where the number of columns don't match the expected number of columns and may cause problems when trying to use the read_csv function.\n",
        "# Create a new CSV file with the valid rows.\n",
        "\n",
        "def remove_invalid_rows(input_file, output_file, delimiter=','):\n",
        "  \"\"\"Removes invalid rows from a CSV file.\n",
        "\n",
        "  Args:\n",
        "    input_file: Path to the input CSV file.\n",
        "    output_file: Path to the output CSV file.\n",
        "    delimiter: Delimiter used in the CSV file.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
        "       open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
        "\n",
        "    reader = csv.reader(infile, delimiter=delimiter)\n",
        "    writer = csv.writer(outfile, delimiter=delimiter)\n",
        "\n",
        "    header = next(reader)  # Read the header row\n",
        "    expected_num_columns = len(header)\n",
        "    writer.writerow(header)  # Write the header to the output file\n",
        "\n",
        "    for row in reader:\n",
        "      if len(row) == expected_num_columns:\n",
        "        writer.writerow(row)\n",
        "\n",
        "# prompt: Create a function that removes invalid rows from a CSV. Detect the number of columns based on the header and the specified delimiter.\n",
        "# Rows where the number of columns don't match the expected number of columns and may cause problems when trying to use the read_csv function.\n",
        "\n",
        "def remove_invalid_rows_from_csv(csv_file_path, delimiter=','):\n",
        "  \"\"\"Removes rows from a CSV file that have an invalid number of columns.\n",
        "\n",
        "  Args:\n",
        "    csv_file_path: The path to the CSV file.\n",
        "    delimiter: The delimiter used in the CSV file.\n",
        "\n",
        "  Returns:\n",
        "    A list of valid rows.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file, delimiter=delimiter)\n",
        "    header = next(reader)  # Get the header row\n",
        "    expected_num_columns = len(header)\n",
        "    valid_rows = [header]  # Start with the header\n",
        "\n",
        "    for row in reader:\n",
        "      if len(row) == expected_num_columns:\n",
        "        valid_rows.append(row)\n",
        "      else:\n",
        "        print(f\"Warning: Skipping row with invalid number of columns: {row}\")\n",
        "\n",
        "  return valid_rows\n",
        "\n",
        "# Example usage:\n",
        "# valid_rows = remove_invalid_rows_from_csv('my_file.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnpCWYqnYXKm"
      },
      "source": [
        "# **Step 1: Functions to run: 1. Drop unneccesary columns. 2. Check for duplicates, then use the valid CSV to create the chunks for further processing.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr4tzL_0WTxL",
        "outputId": "f48c9c08-3bf8-41b8-9e35-685a0745b3f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing CSV file (/content/car-owners-china-v2.csv). Columns to be dropped are ['gender', 'industry', 'monthly_salary', 'marital_status', 'education', 'brand', 'car_series', 'car_model', 'configuration', 'color', 'engine_number', 'Unnamed: 21']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-9eac079a0a0f>:21: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(input_csv, encoding='utf-8',low_memory=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file (/content/car-owners-china-v2.csv) processed successfully. Output saved to car-owners-china-v3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-9eac079a0a0f>:76: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, sep=sep, low_memory=True, encoding='utf-8')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing CSV file: /content/car-owners-china-v3.csv for duplicates.\n",
            "Processing dataframe to remove and store duplicate records.\n",
            "Duplicate removal complete. Duplicate records appended to duplicate_df.\n",
            "Processed file: /content/car-owners-china-v3.csv. Valid data saved to car-owners-china_valid.csv, duplicates to car-owners-china_duplicate_data.csv.\n"
          ]
        }
      ],
      "source": [
        "# Drop unneccesary columns:\n",
        "input_csv = '/content/car-owners-china-v2.csv'\n",
        "output_csv = 'car-owners-china-v3.csv'\n",
        "columns_to_drop = ['gender', 'industry', 'monthly_salary', 'marital_status', 'education', 'brand', 'car_series', 'car_model', 'configuration', 'color', 'engine_number','Unnamed: 21']\n",
        "process_drop_cols_csv(input_csv, output_csv, columns_to_drop)\n",
        "# List of all columns:'vehicle_identification_number', 'name', 'id_card_number', 'gender', 'mobile_phone', 'email', 'province', 'city', 'address', 'postal_code', 'date_of_birth', 'industry', 'monthly_salary', 'marital_status', 'education', 'brand', 'car_series', 'car_model', 'configuration', 'color', 'engine_number'\n",
        "\n",
        "# Check for Duplicates in the Original CSV:\n",
        "process_duplicates_csv('/content/car-owners-china-v3.csv', 'car-owners-china_valid.csv', 'car-owners-china_duplicate_data.csv', sep=',', columns=['vehicle_identification_number','name', 'id_card_number'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNpxcFzhuAY8"
      },
      "source": [
        "# ** Step 2: Function to split chunks based on the chunksize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHk3n7fvvrUU",
        "outputId": "2cd92097-d382-457f-d314-0307a5098a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing CSV file: /content/car-owners-china_valid.csv to split into chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-9eac079a0a0f>:112: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunksize, sep=sep, encoding='utf-8')):\n",
            "ERROR:root:An unexpected error occurred when splitting CSV into chunks: 'module' object is not callable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File '/content/car-owners-china_valid.csv' split into 3 chunks in '/content/chunks'.\n",
            "An unexpected error occurred when splitting CSV into chunks: 'module' object is not callable\n"
          ]
        }
      ],
      "source": [
        "# Split the large CSV into chunks for further processing:\n",
        "# Replace 'your_large_file.csv' with the actual path to your file\n",
        "# Replace 'output_chunks_folder' with the desired output directory\n",
        "#split_csv_into_chunks('/content/lifebear.csv', 1000000, '/content/chunks', sep=';')\n",
        "split_csv_into_chunks('/content/car-owners-china_valid.csv', 250000, '/content/chunks', sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru6h0nJOSnsT"
      },
      "source": [
        "# *Step 3: Run data cleaning functions and export chunks to specified output folders*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63KGWBgBSoFm",
        "outputId": "412a56b8-fe75-4bf5-d8ab-38f5cbb65e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunks from: /content/chunks for data cleaning and output to folders.\n",
            "Processing file: /content/chunks/chunk_3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-9eac079a0a0f>:154: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, low_memory=True, encoding='utf-8')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating emails in column 'email'.\n",
            "Finished validating emails in column 'email'.\n",
            "Combining columns: ['address', 'province', 'city', 'postal_code'] into full_address.\n",
            "Combined columns:['address', 'province', 'city', 'postal_code'] successfully.\n",
            "Finished validating column 'vehicle_identification_number'. Invalid rows found: 0.\n",
            "Finished validating column 'id_card_number'. Invalid rows found: 1.\n",
            "Final valid data saved to /content/cleaned_chunks/valid_chunk_3.csv.\n",
            "Final error data saved to /content/error_chunks/error_chunk_3.csv.\n",
            "File from /content/chunks/chunk_3.csv processed successfully.\n",
            "Processing file: /content/chunks/chunk_1.csv\n",
            "Validating emails in column 'email'.\n",
            "Finished validating emails in column 'email'.\n",
            "Combining columns: ['address', 'province', 'city', 'postal_code'] into full_address.\n",
            "Combined columns:['address', 'province', 'city', 'postal_code'] successfully.\n",
            "Finished validating column 'vehicle_identification_number'. Invalid rows found: 0.\n",
            "Finished validating column 'id_card_number'. Invalid rows found: 0.\n",
            "Final valid data saved to /content/cleaned_chunks/valid_chunk_1.csv.\n",
            "Final error data saved to /content/error_chunks/error_chunk_1.csv.\n",
            "File from /content/chunks/chunk_1.csv processed successfully.\n",
            "Processing file: /content/chunks/chunk_2.csv\n",
            "Validating emails in column 'email'.\n",
            "Finished validating emails in column 'email'.\n",
            "Combining columns: ['address', 'province', 'city', 'postal_code'] into full_address.\n",
            "Combined columns:['address', 'province', 'city', 'postal_code'] successfully.\n",
            "Finished validating column 'vehicle_identification_number'. Invalid rows found: 0.\n",
            "Finished validating column 'id_card_number'. Invalid rows found: 0.\n",
            "Final valid data saved to /content/cleaned_chunks/valid_chunk_2.csv.\n",
            "Final error data saved to /content/error_chunks/error_chunk_2.csv.\n",
            "File from /content/chunks/chunk_2.csv processed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Run data cleaning functions and export chunks to specified output folders:\n",
        "process_chunked_csvs_output_folders('/content/chunks', '/content/cleaned_chunks', '/content/error_chunks')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_2YKkzcqnPY"
      },
      "source": [
        "# *Step 3 (Alternate): Run data cleaning functions and combine chunks to specified CSVs*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6yo-iqBW3_f",
        "outputId": "fb9077e6-047a-4ae4-a957-4fe918b0bebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-9471dffa8647>:258: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, low_memory=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation complete. Invalid email records appended to error_df.\n",
            "Validation complete. Invalid email records appended to error_df.\n"
          ]
        }
      ],
      "source": [
        "# Get chunked csvs from a specified folder, runs the validation functions and merges the chunks into a specified final valid csv file and final error csv file. INclude error checking and logging.\n",
        "# Run data cleaning functions and combine chunks to specified CSVs\n",
        "process_chunked_csvs('/content/chunks', 'final_valid_data.csv', 'final_error_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WaIuBaHfR3K"
      },
      "source": [
        "# Step 4 (Optional): Function to Combine Chunks into a Single CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luRHf5DifQgr",
        "outputId": "64d05d3c-5165-4eab-a12d-39005c9fd0f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combining CSV chunks from /content/cleaned_chunks\n",
            "Combined CSV chunks saved to combined_cleaned_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Combine chunks into a single CSV:\n",
        "# Replace 'your_chunks_folder' and 'combined_file.csv' with your actual paths\n",
        "combine_csv_chunks('/content/cleaned_chunks', 'combined_cleaned_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogTchYHSwt3X",
        "outputId": "e0b7439f-d405-489a-c8f2-81efb05590c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-8c5b2044843f>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/final_valid_data.csv')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "   created_at          salt birthday_on  gender  \n",
            "0  2012-01-13  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17  swFznWWk79fg  1986-10-21     0.0  \n"
          ]
        }
      ],
      "source": [
        "# prompt: Create code to read a csv and show sample of dataframe\n",
        "\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('/content/final_valid_data.csv')\n",
        "\n",
        "# Show a sample of the dataframe (e.g., the first 5 rows)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFnKqa7nAuAG",
        "outputId": "031a5bd9-6230-45e2-f244-fd7259ef2d31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-07609f5dc364>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/lifebear.csv', sep=\";\", low_memory=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               id         login_id                    mail_address  \\\n",
            "136           138        maaam1120             ammma1120@gmail.com   \n",
            "221           223         exuernok            2br02b1215@gmail.com   \n",
            "227           229     takayuki0930          takayuki0930@gmail.com   \n",
            "332           334           hiromi       hiromi.sono.111@gmail.com   \n",
            "621           623          UZUMAME       uzumame.uzumame@gmail.com   \n",
            "...           ...              ...                             ...   \n",
            "3679894  11593957          0enaka0           dara0o0arad@gmail.com   \n",
            "3679926  11594071         ns109097             ns109097@icloud.com   \n",
            "3679964  11594208      takayamasae        as_coco_0520@yahoo.co.jp   \n",
            "3680042  11594511          hyx0630             hyx0630@ezweb.ne.jp   \n",
            "3680353  11595638  relapishoptest9  relapishoptest9@lifebear.co.jp   \n",
            "\n",
            "                                 password           created_at          salt  \\\n",
            "136      f2dea97eab78a50d6cc615c2f172c890  2012-05-28 11:58:21  48dpW9JT7u1w   \n",
            "221      214a676dc79cddbaa42b25a91b0409f0  2012-06-02 04:45:38  FtBEDlKNBr6Z   \n",
            "227      157c0fc86ae997f44a92ade83f56d3e0  2012-06-04 10:48:38  QVsw2y4RgI4o   \n",
            "332      72a6b960035a9d5438b4d09614da6bdb  2012-07-27 23:30:32  t5m1mYhRmbO3   \n",
            "621      2177d847a7e8cd37ed321637a86ce66c  2012-07-29 22:24:48  q90atNIh5iV9   \n",
            "...                                   ...                  ...           ...   \n",
            "3679894  7f1dec1c78f4f63ac4849911a7856ade  2019-02-21 00:26:34  yWbfrArUapNA   \n",
            "3679926  22650ec4dd2951d15277adc82180512d  2019-02-21 00:53:41  cHQlQyZa2acX   \n",
            "3679964  70dd2e962bec6c420c3b9753cd701800  2019-02-21 01:40:27  0W7RlMpljZHT   \n",
            "3680042  66a02d47b178164266e4b8a6593ec81e  2019-02-21 07:10:00  SELwhAAPwOgS   \n",
            "3680353  faa17f36e63271e1c0f50d42cb57509d  2019-02-21 14:42:20  pJsuJpLNsXm0   \n",
            "\n",
            "        birthday_on  gender  \n",
            "136      1983-11-20     0.0  \n",
            "221      1986-12-15     0.0  \n",
            "227      1982-09-30     0.0  \n",
            "332      1976-01-11     1.0  \n",
            "621      1996-07-05     1.0  \n",
            "...             ...     ...  \n",
            "3679894         NaN     NaN  \n",
            "3679926         NaN     1.0  \n",
            "3679964         NaN     1.0  \n",
            "3680042         NaN     NaN  \n",
            "3680353         NaN     NaN  \n",
            "\n",
            "[17870 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "# prompt: Generate code to show all duplicate mail_address from lifebear.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'lifebear.csv' with the actual path to your CSV file\n",
        "df = pd.read_csv('/content/lifebear.csv', sep=\";\", low_memory=True)\n",
        "\n",
        "# Check if 'mail_address' exists in the DataFrame\n",
        "if 'mail_address' in df.columns:\n",
        "    # Find duplicate mail_address entries\n",
        "    duplicate_emails = df[df.duplicated(subset=['mail_address'], keep=False)]  # keep=False shows all duplicates\n",
        "    # Print the duplicate email addresses\n",
        "    print(duplicate_emails)\n",
        "else:\n",
        "    print(\"The 'mail_address' column does not exist in the DataFrame.\")\n",
        "\n",
        "duplicate_emails.to_csv('duplicate_emails.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
